{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tiền xử lý dữ liệu</h2>\n",
    "<img class=\"alignnone size-full wp-image-10398 aligncenter\" src=\"https://ongxuanhong.files.wordpress.com/2016/12/wordcloud.png\" alt=\"wordcloud.png\" width=\"480\" height=\"480\" />\n",
    "\n",
    "Khi bạn download dữ liệu về, <strong>food.txt</strong> sẽ có dạng\n",
    "\n",
    "```\n",
    "product/productId: B001E4KFG0\n",
    "review/userId: A3SGXH7AUHU8GW\n",
    "review/profileName: delmartian\n",
    "review/helpfulness: 1/1\n",
    "review/score: 5.0\n",
    "review/time: 1303862400\n",
    "review/summary: Good Quality Dog Food\n",
    "review/text: I have bought several of the Vitality canned dog food products and have\n",
    "found them all to be of good quality. The product looks more like a stew than a\n",
    "processed meat and it smells better. My Labrador is finicky and she appreciates this\n",
    "product better than most.\n",
    "```\n",
    "\n",
    "Ở đây, ta chỉ quan tâm đến các thông tin\n",
    "<ul>\n",
    "    <li>product/productId là mã sản phẩm</li>\n",
    "    <li>review/score là điểm đánh giá của người dùng cụ thể</li>\n",
    "    <li>review/summary là đánh giá chung về sản phẩm</li>\n",
    "    <li>review/text là phản hồi của người dùng về sản phẩm</li>\n",
    "</ul>\n",
    "Kiểm tra thông tin file bằng dòng lệnh, tập dữ liệu này có hơn 500,000 dòng, hơn 56 triệu từ, và có kích thước khoảng 370 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5116093 56624549 370796484\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/foods.txt | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368M\tdata/foods.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du -h data/foods.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta sẽ viết hàm <strong>convert_plain_to_csv()</strong> để chuyển đổi định dạng plain text ban đầu thành .csv\n",
    "<ul>\n",
    "    <li>Đọc file food.txt</li>\n",
    "    <li>Mỗi lần đọc ra 9 dòng</li>\n",
    "    <li>Ghép các thông tin <strong>productId,score,summary,text</strong> thành một dòng ngăn cách với nhau bởi dấu \",\"</li>\n",
    "    <li>Riêng summary và text ta sẽ làm sạch dữ liệu:\n",
    "<ul>\n",
    "    <li>Loại bỏ các HTML tags</li>\n",
    "    <li>Loại bỏ các kí số, kí tự đặc biệt như dấu chấm, dấu phẩy,...</li>\n",
    "</ul>\n",
    "</li>\n",
    "    <li>Lưu lại vào file <strong>food.csv</strong></li>\n",
    "    <li>Tổng thời gian chuyển đổi khoảng 5 phút với cấu hình Macbook như bên dưới</li>\n",
    "</ul>\n",
    "<img class=\"alignnone size-full wp-image-10421\" src=\"https://ongxuanhong.files.wordpress.com/2016/12/about-macbook.png\" alt=\"about macbook.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLASSIFICATION\n",
    "Case study: Analyzing sentiment\n",
    "Models:\n",
    "    Linear classifiers (logistic regression, SVMs, perceptron)\n",
    "    Kernels\n",
    "    Decision trees\n",
    "Algorithms:\n",
    "    Stochastic gradient descent\n",
    "    Boosting\n",
    "Concepts:\n",
    "    Decision boundaries, MLE, ensemble methods, random forests, CART, online learning\n",
    "\"\"\"\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_diff_str(t1, t2):\n",
    "    \"\"\"\n",
    "    Calculates time durations.\n",
    "    \"\"\"\n",
    "    diff = t2 - t1\n",
    "    mins = int(diff / 60)\n",
    "    secs = round(diff % 60, 2)\n",
    "    return str(mins) + \" mins and \" + str(secs) + \" seconds\"\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(sentence).text\n",
    "\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    return letters_only\n",
    "\n",
    "def convert_plain_to_csv(plain_name, tsv_name):\n",
    "    t0 = time.time()\n",
    "    with open(plain_name, \"r\") as f1, open(tsv_name, \"w\") as f2:\n",
    "        i = 0\n",
    "        f2.write(\"productId,score,summary,text\\n\")\n",
    "        while True:\n",
    "            next_n_lines = list(islice(f1, 9))\n",
    "            if not next_n_lines:\n",
    "                break\n",
    "\n",
    "            # process next_n_lines: get productId,score,summary,text info\n",
    "            # remove special characters from summary and text\n",
    "            output_line = \"\"\n",
    "            for line in next_n_lines:\n",
    "                if \"product/productId:\" in line:\n",
    "                    output_line += line.split(\":\")[1].strip() + \",\"\n",
    "                elif \"review/score:\" in line:\n",
    "                    output_line += line.split(\":\")[1].strip() + \",\"\n",
    "                elif \"review/summary:\" in line:\n",
    "                    summary = clean_sentence(line.split(\":\")[1].strip()) + \",\"\n",
    "                    output_line += summary\n",
    "                elif \"review/text:\" in line:\n",
    "                    text = clean_sentence(line.split(\":\")[1].strip()) + \"\\n\"\n",
    "                    output_line += text\n",
    "\n",
    "            f2.write(output_line)\n",
    "\n",
    "            # print status\n",
    "            i += 1\n",
    "            if i % 20000 == 0:\n",
    "                print \"%d reviews converted...\" % i\n",
    "\n",
    "    print \" %s - Converting completed %s\" % (datetime.datetime.now(), time_diff_str(t0, time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 reviews converted...\n",
      "40000 reviews converted...\n",
      "60000 reviews converted...\n",
      "80000 reviews converted...\n",
      "100000 reviews converted...\n",
      "120000 reviews converted...\n",
      "140000 reviews converted...\n",
      "160000 reviews converted...\n",
      "180000 reviews converted...\n",
      "200000 reviews converted...\n",
      "220000 reviews converted...\n",
      "240000 reviews converted...\n",
      "260000 reviews converted...\n",
      "280000 reviews converted...\n",
      "300000 reviews converted...\n",
      "320000 reviews converted...\n",
      "340000 reviews converted...\n",
      "360000 reviews converted...\n",
      "380000 reviews converted...\n",
      "400000 reviews converted...\n",
      "420000 reviews converted...\n",
      "440000 reviews converted...\n",
      "460000 reviews converted...\n",
      "480000 reviews converted...\n",
      "500000 reviews converted...\n",
      "520000 reviews converted...\n",
      "540000 reviews converted...\n",
      "560000 reviews converted...\n",
      " 2018-05-04 12:33:44.530843 - Converting completed 2 mins and 53.04 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pre-processing\n",
    "\"\"\"\n",
    "# converting plain text for next processing\n",
    "in_file = \"data/foods.txt\"\n",
    "out_file = \"data/foods.csv\"\n",
    "clean_file = \"data/clean_train_reviews.csv\"\n",
    "convert_plain_to_csv(in_file, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta tiến hành quan sát tập dữ liệu vừa mới chuyển đổi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- data/foods.csv found locally\n",
      "Data dimensions: (568454, 4)\n",
      "List features: ['productId' 'score' 'summary' 'text']\n",
      "First review: Good Quality Dog Food | I have bought several of the Vitality canned dog food products and have found them all to be of good quality  The product looks more like a stew than a processed meat and it smells better  My Labrador is finicky and she appreciates this product better than  most \n"
     ]
    }
   ],
   "source": [
    "def get_reviews_data(file_name):\n",
    "    \"\"\"Get reviews data, from local csv.\"\"\"\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"-- \" + file_name + \" found locally\")\n",
    "        df = pd.read_csv(file_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Reading the Data\n",
    "train = get_reviews_data(out_file)\n",
    "print \"Data dimensions:\", train.shape\n",
    "print \"List features:\", train.columns.values\n",
    "print \"First review:\", train[\"summary\"][0], \"|\", train[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, ta sẽ loại bỏ các từ không liên quan bằng danh sách <a href=\"http://www.nltk.org/howto/corpus.html#word-lists-and-lexicons\" target=\"_blank\" rel=\"noopener\">stopwords</a> trong tiếng Anh. Đây là những từ hay xuất hiện trong câu nhưng không góp phần vào quá trình học của hệ thống như <strong>\"a\", \"and\", \"is\"</strong> hoặc <strong>\"the\"</strong>. Với mỗi dòng dữ liệu quan sát ta tiến hành loại bỏ như sau:\n",
    "<ul>\n",
    "    <li>Chuyển sang chữ thường và tách thành danh sách các từ riêng biệt.</li>\n",
    "    <li>Sử dụng stopwords để lọc ra danh sách các từ có ý nghĩa.</li>\n",
    "    <li>Lưu lại vào file <strong>clean_train_reviews.csv</strong></li>\n",
    "    <li>Quá trình này mất khoảng 4 phút.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\"\n",
    "    Function to convert a raw review to a string of words\n",
    "    :param review\n",
    "    :return: meaningful_words\n",
    "    \"\"\"\n",
    "    # 1. Convert to lower case, split into individual words\n",
    "    words = review.lower().split()\n",
    "    #\n",
    "    # 2. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    #\n",
    "    # 3. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 4. Join the words back into one string separated by space,\n",
    "    # and return the result.\n",
    "    return \" \".join(meaningful_words)\n",
    "\n",
    "def cleaning_data(dataset, file_name):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Get the number of reviews based on the dataframe column size\n",
    "    num_reviews = dataset[\"text\"].size\n",
    "\n",
    "    # Initialize an empty list to hold the clean reviews\n",
    "    clean_train_reviews = []\n",
    "\n",
    "    # Loop over each review\n",
    "    for i in xrange(0, num_reviews):\n",
    "        # If the index is evenly divisible by 1000, print a message\n",
    "        if (i + 1) % 20000 == 0:\n",
    "            print \"Review %d of %d\\n\" % (i + 1, num_reviews)\n",
    "\n",
    "        # Call our function for each one, and add the result to the list of\n",
    "        # clean reviews\n",
    "        productId = str(dataset[\"productId\"][i])\n",
    "        score = str(dataset[\"score\"][i])\n",
    "        summary = str(dataset[\"summary\"][i])\n",
    "        text = review_to_words(str(dataset[\"text\"][i]))\n",
    "\n",
    "        clean_train_reviews.append(productId + \",\" + score + \",\" + summary + \",\" + text + \"\\n\")\n",
    "\n",
    "    print \"Writing clean train reviews...\"\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(\"productId,score,summary,text\\n\")\n",
    "        for review in clean_train_reviews:\n",
    "            f.write(\"%s\\n\" % review)\n",
    "\n",
    "    print \" %s - Write file completed %s\" % (datetime.datetime.now(), time_diff_str(t0, time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 20000 of 568454\n",
      "\n",
      "Review 40000 of 568454\n",
      "\n",
      "Review 60000 of 568454\n",
      "\n",
      "Review 80000 of 568454\n",
      "\n",
      "Review 100000 of 568454\n",
      "\n",
      "Review 120000 of 568454\n",
      "\n",
      "Review 140000 of 568454\n",
      "\n",
      "Review 160000 of 568454\n",
      "\n",
      "Review 180000 of 568454\n",
      "\n",
      "Review 200000 of 568454\n",
      "\n",
      "Review 220000 of 568454\n",
      "\n",
      "Review 240000 of 568454\n",
      "\n",
      "Review 260000 of 568454\n",
      "\n",
      "Review 280000 of 568454\n",
      "\n",
      "Review 300000 of 568454\n",
      "\n",
      "Review 320000 of 568454\n",
      "\n",
      "Review 340000 of 568454\n",
      "\n",
      "Review 360000 of 568454\n",
      "\n",
      "Review 380000 of 568454\n",
      "\n",
      "Review 400000 of 568454\n",
      "\n",
      "Review 420000 of 568454\n",
      "\n",
      "Review 440000 of 568454\n",
      "\n",
      "Review 460000 of 568454\n",
      "\n",
      "Review 480000 of 568454\n",
      "\n",
      "Review 500000 of 568454\n",
      "\n",
      "Review 520000 of 568454\n",
      "\n",
      "Review 540000 of 568454\n",
      "\n",
      "Review 560000 of 568454\n",
      "\n",
      "Writing clean train reviews...\n",
      " 2018-05-04 12:36:50.457401 - Write file completed 2 mins and 20.03 seconds\n"
     ]
    }
   ],
   "source": [
    "cleaning_data(train, clean_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sử dụng Bag of words để tạo features</h2>\n",
    "\n",
    "Bag of Words model xây dựng bộ từ vựng thông qua tập các văn bản, sau đó mô hình hóa từng văn bản (vector hóa) bằng cách đếm số lần xuất hiện của các từ xuất hiện trong văn bản đó. Ví dụ, ta có hai câu sau:\n",
    "<ul>\n",
    "    <li>Câu 1: \"The cat sat on the hat\"</li>\n",
    "    <li>Câu 2: \"The dog ate the cat and the hat\"</li>\n",
    "</ul>\n",
    "Từ hai câu trên, bộ từ vựng của chúng ta sẽ là\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Để có bags of words, ta sẽ đếm số lần xuất hiện của từng từ trong từng câu. Trong câu 1, \"the\" xuất hiện 2 lần, các từ \"cat\", \"sat\", \"on\", và \"hat\" đề xuất hiện 1 lần, nên ta có feature vector cho câu 1 là\n",
    "<ul>\n",
    "    <li>Câu 1: { 2, 1, 1, 1, 1, 0, 0, 0 }</li>\n",
    "    <li>Tương tự cho câu 2: { 3, 1, 0, 0, 1, 1, 1, 1}</li>\n",
    "</ul>\n",
    "Ta tiến hành vector hóa cho tập dữ liệu đã được xử lý\n",
    "<ul>\n",
    "    <li>Do dữ liệu khá lớn, nên ta sẽ lấy khoảng 1000 dòng quan sát để thực nghiệm. Khi mọi thứ đã chạy ổn, ta sẽ chạy lại cho tất cả để rút ra được mô hình cuối cùng cho hệ thống.</li>\n",
    "    <li>Ta sẽ loại bỏ bớt các review có điểm = 3.0 để phân biệt rõ ràng hơn giữa phản hồi tích cực (positive) và tiêu cực (negative).</li>\n",
    "    <li>Ở đây, ta đánh giá một phản hồi là tích cực khi có điểm đánh giá >= 4.0</li>\n",
    "    <li>Tiếp theo, ta sẽ phân chia tập dữ liệu train và test theo tỉ lệ 80/20</li>\n",
    "    <li><strong>CountVectorizer</strong> được dùng để phát sinh vector Bag of Words</li>\n",
    "    <li>Cuối cùng, ta sử dụng hàm <strong>fit_transform()</strong> để chuyển đổi thành ma trận term-document làm input cho các hàm phân lớp.</li>\n",
    "    <li>Ngoài ra, bạn có thể gọi hàm <strong>print_words_frequency()</strong> để in ra danh sách các từ kèm tần suất xuất hiện của chúng.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Words in vocabulary: [u'bag', u'chips', u'flavor', u'food', u'good', u'great', u'kettle', u'like', u'one', u'taste']\n",
      "Words frequency...\n",
      "140 bag\n",
      "365 chips\n",
      "194 flavor\n",
      "158 food\n",
      "228 good\n",
      "207 great\n",
      "284 kettle\n",
      "167 like\n",
      "190 one\n",
      "200 taste\n"
     ]
    }
   ],
   "source": [
    "def print_words_frequency(train_data_features):\n",
    "    # Take a look at the words in the vocabulary\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    print \"Words in vocabulary:\", vocab\n",
    "\n",
    "    # Sum up the counts of each vocabulary word\n",
    "    dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "    # For each, print the vocabulary word and the number of times it\n",
    "    # appears in the training set\n",
    "    print \"Words frequency...\"\n",
    "    for tag, count in zip(vocab, dist):\n",
    "        print count, tag\n",
    "\n",
    "clean_train_reviews = pd.read_csv(clean_file, nrows=1000)\n",
    "\n",
    "# ignore all 3* reviews\n",
    "clean_train_reviews = clean_train_reviews[clean_train_reviews[\"score\"] != 3]\n",
    "# positive sentiment = 4* or 5* reviews\n",
    "clean_train_reviews[\"sentiment\"] = clean_train_reviews[\"score\"] >= 4\n",
    "\n",
    "train, test = train_test_split(clean_train_reviews, test_size=0.2)\n",
    "\n",
    "print \"Creating the bag of words...\\n\"\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=None,\n",
    "                             max_features=10)\n",
    "\n",
    "train_text = train[\"text\"].values.astype('U')\n",
    "test_text = test[\"text\"].values.astype('U')\n",
    "\n",
    "# convert data-set to term-document matrix\n",
    "X_train = vectorizer.fit_transform(train_text).toarray()\n",
    "y_train = train[\"sentiment\"]\n",
    "\n",
    "X_test = vectorizer.fit_transform(test_text).toarray()\n",
    "y_test = test[\"sentiment\"]\n",
    "\n",
    "print_words_frequency(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "\n",
    "Đây có lẽ là bước mọi người mong chờ nhất. Ta sẽ sử dụng tập các hàm phân lớp khác nhau và để chọn ra được mô hình cho kết quả chính xác cao nhất.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Nearest Neighbors classifier...\n",
      "Training Linear SVM classifier...\n",
      "Training RBF SVM classifier...\n",
      "Training Gaussian Process classifier...\n",
      "Training Decision Tree classifier...\n",
      "Training Random Forest classifier...\n",
      "Training Neural Net classifier...\n",
      "Training AdaBoost classifier...\n",
      "Training Naive Bayes classifier...\n",
      "Training QDA classifier...\n",
      "---------------------------\n",
      "Evaluation results\n",
      "---------------------------\n",
      "Gaussian Process accuracy: 0.816\n",
      "Decision Tree accuracy: 0.822\n",
      "QDA accuracy: 0.768\n",
      "Naive Bayes accuracy: 0.768\n",
      "Linear SVM accuracy: 0.816\n",
      "Neural Net accuracy: 0.816\n",
      "RBF SVM accuracy: 0.805\n",
      "AdaBoost accuracy: 0.816\n",
      "Random Forest accuracy: 0.816\n",
      "Nearest Neighbors accuracy: 0.805\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "             \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "             \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# iterate over classifiers\n",
    "results = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print \"Training \" + name + \" classifier...\"\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    results[name] = score\n",
    "\n",
    "print \"---------------------------\"\n",
    "print \"Evaluation results\"\n",
    "print \"---------------------------\"\n",
    "\n",
    "# sorting results and print out\n",
    "sorted(results.items(), key=itemgetter(1))\n",
    "for name in results:\n",
    "    print name + \" accuracy: %0.3f\" % results[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Kết</h2>\n",
    "\n",
    "Nếu bạn đã làm theo tutorial đến được đây và chạy chương trình thành công thì xin chúc mừng, bạn đã xây dựng được cho mình một hệ thống Sentiment Analysis cơ bản. Bạn có thể thử nhiều cách khác nhau để cải thiện độ chính xác của mô hình. Bạn có thể làm sạch dữ liệu theo nhiều cách khác như giữ lại các biểu tượng cảm xúc hay các từ viết tắt phổ biến, thay đổi số lượng từ vựng của Bag of Words để xem độ chính xác có thay đổi không.\n",
    "\n",
    "Ngoài ra bạn cũng nên áp dụng kỹ thuật này lên các tập dữ liệu khác để tạo ra nhiều ứng dụng thú vị hơn như:\n",
    "<ul>\n",
    "    <li>Chọn ra nhà hàng nào được review tốt nhất trong thành phố của bạn để ghé ăn cuối tuần.</li>\n",
    "    <li>Thu thập dữ liệu từ một trang web ăn uống và xây dựng cho mình ứng dụng phân tích review các món ăn.</li>\n",
    "    <li>Thu thập dữ liệu từ các diễn đàn iPhone và Samsung thử đoán xem sản phẩm nào được đánh giá cao hơn khi áp dụng hệ thống của bạn.</li>\n",
    "    <li>Xây dựng ứng dụng đánh giá phim, bài hát tự động từ những phản hồi của người dùng.</li>\n",
    "    <li>Áp dụng cho tiếng Việt bằng cách sử dụng các thư viện do cộng đồng Xử lý ngôn ngữ tự nhiên Việt Nam đóng góp như:\n",
    "<ul>\n",
    "    <li><a href=\"http://dl.acm.org/citation.cfm?id=2542062&dl=ACM&coll=DL&CFID=871395999&CFTOKEN=64946196\" target=\"_blank\" rel=\"noopener\">VNLP</a>: an open source framework for Vietnamese natural language processing</li>\n",
    "    <li><a href=\"http://jvntextpro.sourceforge.net\" target=\"_blank\" rel=\"noopener\">JVnTextPro</a>: A Java-based Vietnamese Text Processing Tool</li>\n",
    "    <li><a href=\"http://mim.hus.vnu.edu.vn/phuonglh/softwares/vnTokenizer\" target=\"_blank\" rel=\"noopener\">vnTokenizer</a>: Vietnamese word segmentation</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "Sau bài viết này, bạn có thể nghiên cứu thêm các vấn đề liên quan như:\n",
    "<ul>\n",
    "    <li>Confusion matrix.</li>\n",
    "    <li>False negative, False positive có ảnh hưởng thế nào đến các ứng dụng như chẩn đoán bệnh hay lọc thư rác?</li>\n",
    "    <li>Thế nào là bias, dữ liệu càng nhiều thì độ chính xác của mô hình có tăng lên không?</li>\n",
    "    <li>Các kĩ thuật đánh giá mô hình khác.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
